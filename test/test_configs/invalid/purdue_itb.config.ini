;===================================================================
;                       IMPORTANT
;===================================================================
;
; 
; You can get documentation on the syntax of this file at:
; https://twiki.grid.iu.edu/twiki/bin/view/Integration/ITB090/ConfigurationFileFormat
; You can get documentation on the options for each section at:
; https://twiki.grid.iu.edu/twiki/bin/view/Integration/ITB090/ConfigurationFileHelp
;


[DEFAULT]
; Use this section to define variables that will be used in other sections
; For example, if you define a variable called dcache_root here
; you can use it in the gip section as %(dcache_root)s  (e.g. 
; my_vo_1_dir = %(dcache_root)s/my_vo_1
; my_vo_2_dir = %(dcache_root)s/my_vo_2

; Defaults, please don't modify these variables
unavailable = UNAVAILABLE
default = UNAVAILABLE

; Name these variables disable and enable rather than disabled and enabled
; to avoid infinite recursions
disable = False
enable = True

; You can modify the following and use them
localhost = itb.rcac.purdue.edu
admin_email = fhu@purdue.edu
osg_location = /opt/osg

;===================================================================
;                       Site Information
;===================================================================

[Site Information]
; The group option indicates the group that the OSG site should be listed in,
; for production sites this should be OSG, for vtb or itb testing it should be
; OSG-ITB
; 
; YOU WILL NEED TO CHANGE THIS
group = OSG-ITB

; The host_name setting should give the host name of the CE  that is being 
; configured, this setting must be a valid dns name that resolves
; 
; YOU WILL NEED TO CHANGE THIS
host_name = itb.rcac.purdue.edu

; The site_name setting should give the registered OSG site name (e.g. OSG_ITB)
; 
; YOU WILL NEED TO CHANGE THIS
site_name = Purdue-ITB

; The sponsor setting should list the sponsors for your cluster, if your cluster
; has multiple sponsors, you can separate them using commas or specify the 
; percentage using the following format 'osg, atlas, cms' or 
; 'osg:10, atlas:45, cms:45'  
; 
; YOU WILL NEED TO CHANGE THIS
sponsor = uscms

; The site_policy setting should give an url that lists your site's usage
; policy 
site_policy = http://www.physics.purdue.edu/tier2/policy

; The contact setting should give the name of the admin/technical contact
; for the cluster
; 
; YOU WILL NEED TO CHANGE THIS
contact = Fengping Hu

; The email setting should give the email address for the technical contact
; for the cluster 
; 
; YOU WILL NEED TO CHANGE THIS
email = fhu@purdue.edu

; The city setting should give the city that the cluster is located in
; 
; YOU WILL NEED TO CHANGE THIS
city = West Lafayette

; The country setting should give the country that the cluster is located in
; 
; YOU WILL NEED TO CHANGE THIS
country = USA

; The longitude setting should give the longitude for the cluster's location
; if you are in the US, this should be negative
; accepted values are between -180 and 180
; 
; YOU WILL NEED TO CHANGE THIS
longitude = -86.911

; The latitude setting should give the latitude for the cluster's location
; accepted values are between -90 and 90
; 
; YOU WILL NEED TO CHANGE THIS
latitude = 40.444


;===================================================================
; For the following job manager sections (FBS, LSF, SGE, PBS, Condor)
; you should delete the sections corresponding to job managers that 
; you are not using.  E.g. if you are just using LSF on your site,
; you can delete 
;===================================================================


;===================================================================
;                              PBS
;===================================================================


[PBS]
; This section has settings for configuring your CE for a PBS job manager

; The enabled setting indicates whether you want your CE to use a PBS job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the pbs install directory
home = %(unavailable)s

; The pbs_location setting should give the location of pbs install directory
; This should be the same as the home setting above
pbs_location = %(home)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-pbs) 
job_contact = %(localhost)s/jobmanager-pbs

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager-pbs

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              FBS
;===================================================================


[FBS]
; This section has settings for configuring your CE for a FBS job manager

; The enabled setting indicates whether you want your CE to use a FBS job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the fbs install directory
home = %(unavailable)s

; The fbs_location setting should give the location of fbs install directory
; This should be the same as the home setting above
fbs_location = %(home)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-fbs) 
job_contact = %(localhost)s/jobmanager-fbs

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager-fbs

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                             Condor
;===================================================================


[Condor]
; This section has settings for configuring your CE for a Condor job manager

; The enabled setting indicates whether you want your CE to use a Condor job 
; manager
; valid answers are True or False
enabled = True

; The home setting should give the location of the condor install directory
home = /opt/condor

; The condor_location setting should give the location of condor install directory
; This should be the same as the home setting above
condor_location = /opt/condor

; The condor_location setting should give the location of condor config file,
; This is typically  etc/condor_config within the condor install directory
condor_config = /opt/condor/etc

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-condor) 
job_contact = itb.rcac.purdue.edu/jobmanager-condor

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = itb.rcac.purdue.edu/jobmanager-condor

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = True

;===================================================================
;                              SGE
;===================================================================


[SGE]
; This section has settings for configuring your CE for a SGE job manager

; The enabled setting indicates whether you want your CE to use a SGE job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the sge install directory
home = %(unavailable)s

; The sge_location setting should give the location of sge install directory
; This should be the same as the home setting above
sge_location = %(home)s

; The sge_root setting should give the location of sge install directory
; This should be the same as the home setting above
sge_root = %(home)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-sge) 
job_contact = %(localhost)s/jobmanager-sge

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager-sge

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              LSF
;===================================================================


[LSF]
; This section has settings for configuring your CE for a LSF job manager

; The enabled setting indicates whether you want your CE to use a LSF job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the lsf install directory
home = %(unavailable)s

; The lsf_location setting should give the location of lsf install directory
; This should be the same as the home setting above
lsf_location = %(home)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-lsf) 
job_contact = %(localhost)s/jobmanager-lsf

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager-lsf

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              Managed Fork
;===================================================================


[Managed Fork]
; The enabled setting indicates whether managed fork is in use on the system
; or not. You should set this to True or False
enabled = True

; The condor_location setting should give the location of condor install directory
; This should be the same as the home setting above
condor_location = /opt/condor

; The condor_location setting should give the location of condor config file,
; This is typically  etc/condor_config within the condor install directory
condor_config = /opt/condor/etc

;===================================================================
;                              Misc Services
;===================================================================


[Misc Services]
; If you have glexec installed on your worker nodes, enter the location
; of the glexec binary in this setting
glexec_location = %(unavailable)s

; If you wish to use the ca certificate update service, set this setting to True, 
; otherwise keep this at false
; Please note that as of OSG 1.0, you have to use the ca cert updater or the rpm
; updates, pacman can not update the ca certs
use_cert_updater = %(disable)s

;===================================================================
;                            RSV
;===================================================================


[RSV]
; The enable option indicates whether rsv should be enable or disabled.  It should
; be set to True or False
enabled = True

; The rsv_user option gives the user that the rsv service should use.  It must
; be a valid unix user account
; 
; If rsv is enabled, and this is blank or set to unavailable it will default to 
; rsvuser
rsv_user = fhu

; The enable_ce_probes option enables or disables the RSV CE probes.  If you enable this,
; you should also set the ce_hosts option as well.
;
; Set this to true or false. 
enable_ce_probes = True

; The ce_hosts options lists the FQDN of the CEs that the RSV CE probes should check. 
; This should be a list of FQDNs separated by a comma (e.g. my.host,my.host2,my.host3)
;
; This must be set if the enable_ce_probes option is enabled.  If this is set to 
; UNAVAILABLE or left blank, then it will default to the hostname setting for this CE
ce_hosts = itb.rcac.purdue.edu

; The enable_gridftp_probes option enables or disables the RSV gridftp probes.  If 
; you enable this, you must also set the ce_hosts or gridftp_hosts option as well.
;
; Set this to True or False. 
enable_gridftp_probes = True

; The gridftp_hosts options lists the FQDN of the gridftp servers that the RSV CE 
; probes should check. This should be a list of FQDNs separated by a comma 
; (e.g. my.host,my.host2,my.host3)
;
; This or ce_hosts must be set if the enable_gridftp_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
gridftp_hosts = itb.rcac.purdue.edu

; The gridftp_dir options gives the directory  on the gridftp servers that the 
; RSV CE  probes should try to write and read from. 
;
; This should be set if the enable_gridftp_probes option is enabled. It will default
; to /tmp if left blank or set to UNAVAILABLE 
gridftp_dir = /tmp

; The enable_gums_probes option enables or disables the RSV gums probes.  If 
; you enable this, you must also set the ce_hosts or gums_hosts option as well.
;
; Set this to True or False. 
enable_gums_probes = %(disable)s

; The gums_hosts options lists the FQDN of the CE that uses GUMS server that the 
; RSV GUMS probes should check. This should be a list of FQDNs separated by a 
; comma (e.g. my.host,my.host2,my.host3)
;
; This or ce_hosts should be set if the enable_gums_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
gums_hosts = %(default)s

; The enable_srm_probes option enables or disables the RSV srm probes.  If 
; you enable this, you must also set the srm_hosts option as well.
;
; Set this to True or False. 
enable_srm_probes = %(disable)s

; The srm_hosts options lists the FQDN of the srm servers that the 
; RSV SRM probes should check. This should be a list of FQDNs separated 
; by a comma (e.g. my.host,my.host2,my.host3)
;
; This or _hosts must be set if the enable_srm_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
srm_hosts = %(default)s

; The srm_dir options gives the directory  on the srm servers that the 
; RSV SRM probes should try to write and read from. 
;
; This must be set if the enable_srm_probes option is enabled. 
srm_dir = %(unavailable)s

; This option gives the webservice path that SRM probes need to along with the 
; host: port. For dcache installations, this should work if left blank or left out. 
; However Bestman-xrootd SEs normally use srm/v2/server as web service path, and so 
; Bestman-xrootd admins will have to pass this option with the appropriate value 
; (for example: "srm/v2/server") for the SRM probes to work on their SE.
srm_webservice_path = %(unavailable)s

; Use the use_service_cert option indicates whether to use a service 
; certificate with rsv 
;
; NOTE: This can't be used if you specify multiple CEs or GUMS hosts
use_service_cert = %(disable)s

; You'll need to set this if you have enabled the use_service_cert.  
; This should point to the public key file (pem) for your service 
; certificate
; 
; If this is left blank or set to UNAVAILABLE  and the use_service_cert 
; setting is enabled, it will default to /etc/grid-security/rsvcert.pem
rsv_cert_file  = %(default)s

; You'll need to set this if you have enabled the use_service_cert.  
; This should point to the private key file (pem) for your service 
; certificate
;
; If this is left blank or set to UNAVAILABLE and the use_service_cert 
; setting is enabled, it will default to /etc/grid-security/rsvkey.pem
rsv_key_file  = %(default)s

; You'll need to set this if you have enabled the use_service_cert.  This 
; should point to the location of the rsv proxy file.
;
; If this is left blank or set to UNAVAILABLE and the use_service_cert 
; setting is enabled, it will default to /tmp/rsvproxy
rsv_proxy_out_file = %(default)s

; If you don't use a service certificate for rsv, you will need to specify a 
; proxy file that RSV should use in the proxy_file setting.
; This needs to be set if use_service_cert is disabled
proxy_file = /tmp/x509up_u50483

; This option will enable RSV record uploading to central RSV collector at the GOC
;
; Set this to True or False
enable_gratia = %(disable)s

; The print_local_time option indicates whether rsv should use local times instead of 
; GMT times in the local web pages produced (NOTE: records uploaded to central RSV 
; collector will still have UTC timestamps)
;
; Set this to True or False
print_local_time = %(disable)s

; The setup_rsv_nagios option indicates whether rsv try to connect to a locat
; nagios instance and report information to it as well
;
; Set this to True or False
setup_rsv_nagios = %(disable)s

; The rsv_nagios_conf_file option indicates the location of the rsv nagios 
; file to use for configuration details.  This is optional
;
rsv_nagios_conf_file = %(unavailable)s

; The setup_rsv_nagios option indicates whether rsv try to create a webpage
; that can be used to view the status of the rsv tests.  Enabling this is 
; highly encouraged.
;
; Set this to True or False
setup_for_apache = True


;===================================================================
;                            Storage 
;===================================================================

[Storage]
;
; Several of these values are constrained and need to be set in a way
; that is consistent with one of the OSG storage models
;
; Please refer to the OSG release documentation for an indepth explanation 
; of the various storage models and the requirements for them

; If you have a SE available for your cluster and wish to make it available 
; to incoming jobs, set se_available to True, otherwise set it to False
se_available = %(disable)s

; If you indicated that you have an se available at your cluster, set default_se to
; the hostname of this SE, otherwise set default_se to UNAVAILABLE
default_se = %(unavailable)s

; The grid_dir setting should point to the directory which holds the files 
; from the OSG worker node package, it should be visible on all of the computer
; nodes (read access is required, worker nodes don't need to be able to write) 
; 
; YOU WILL NEED TO CHANGE THIS
grid_dir = /opt/osg

; The app_dir setting should point to the directory which contains the VO 
; specific applications, this should be visible on both the CE and worker nodes
; but only the CE needs to have write access to this directory
; 
; YOU WILL NEED TO CHANGE THIS
app_dir = /apps/osg

; The data_dir setting should point to a directory that can be used to store 
; and stage data in and out of the cluster.  This directory should be readable
; and writable on both the CE and worker nodes
; 
; YOU WILL NEED TO CHANGE THIS
data_dir = /scratch/osg

; The worker_node_temp directory should point to a directory that can be used 
; as scratch space on compute nodes, it should allow read and write access on the 
; worker nodes but can be local to each worker node
; 
; YOU WILL NEED TO CHANGE THIS
worker_node_temp = /tmp

; The site_read setting should be the location or url to a directory that can 
; be read to stage in data, this is an url if you are using a SE 
; 
; YOU WILL NEED TO CHANGE THIS
site_read = /data

; The site_write setting should be the location or url to a directory that can 
; be write to stage out data, this is an url if you are using a SE 
; 
; YOU WILL NEED TO CHANGE THIS
site_write = /data

;===================================================================
;                              Monalisa
;===================================================================

[MonaLisa]
; Set the enabled setting to True if you have monalisa installed and wish to 
; use it, otherwise set it to False 
enabled = %(disable)s

; If you want monalisa to use it's vo modules, set the use_vo_modules setting
; to true, otherwise set this to False
use_vo_modules = %(enable)s

; The ganglia_support setting should be enabled if you are using ganglia on
; your cluster and you wish monalisa to use it as well
ganglia_support = %(disable)s

; If you've enabled ganglia support, you should enter the hostname of the 
; ganglia server in the ganglia_host option
ganglia_host = %(unavailable)s

; If you've enabled ganglia support, you should enter the port that ganglia
; is running on
ganglia_port = %(default)s

;===================================================================
;                             Squid
;===================================================================

[Squid]
; Set the enabled setting to True if you have squid installed and wish to 
; use it, otherwise set it to False 
enabled = %(disable)s

; If you are using squid, specify the location of the squid server in the 
; location setting, this can be a path if squid is installed on the same
; server as the CE or it can be a hostname
location = %(unavailable)s

; If you are using squid, use the policy setting to indicate which cache
; replacement policy squid is using
policy = %(unavailable)s

; If you are using squid, use the cache_size setting to indicate which the 
; size of the disk cache that squid is using
cache_size = %(unavailable)s

; If you are using squid, use the memory_size setting to indicate which the 
; size of the memory cache that squid is using
memory_size = %(unavailable)s


;===================================================================
;                              GIP
;===================================================================

[GIP]

; ========= These settings must be changed ==============

;; This setting indicates the batch system that GIP should query
;; and advertise
;; This should be the name of the batch system in lowercase
batch = condor
;; Options include: pbs, lsf, sge, or condor

; ========= These settings can be left as is for the standard install ========

;; This setting indicates whether GIP should advertise a gsiftp server
;; in addition to a srm server, if you don't have a srm server, this should
;; be enabled
;; Valid options are True or False
advertise_gsiftp = %(enable)s

;; This should be the hostname of the gsiftp server that gip will advertise
gsiftp_host = itb.rcac.purdue.edu

;; This setting indicates whether GIP should query the gums server.
;; Valid options are True or False
advertise_gums = %(disable)s

;
; NOTE ABOUT PREVIOUS GIP OPTIONS:
; There used to be many more options in the GIP section, mostly involving the
; configuration of the site's subclusters and storage elements.  These options
; have been moved into separate sections - one section per subcluster or SE.
; However, backward compatibility with the older format has been retained, and
;

;===================================================================
;                          Subclusters
;===================================================================

; For each subcluster, add a new subcluster section.
; Each subcluster name must be unique for the entire grid, so make sure to not
; pick anything generic like "MAIN".  Each subcluster section must start with
; the words "Subcluster", and cannot be named "CHANGEME".

; There should be one subcluster section per set of homogeneous nodes in the
; cluster.

; This data is used for our statistics collections in the OSG, so it's important
; to keep it up to date.  This is important for WLCG sites as it will be used
; to determine your progress toward your MoU commitments!

; If you have many similar subclusters, then feel free to collapse them into
; larger, approximately-correct groups.

; See example below:

[Subcluster itb.rcac.purdue.edu-xeons]
; should be the name of the subcluster
name = itb.rcac.purdue.edu-xeons
; number of homogeneous nodes in the subcluster
node_count = 70
; Megabytes of RAM per node.
ram_mb = 4096
; CPU model, as taken from /proc/cpuinfo.  Please, no abbreviations!
cpu_model = Dual-Core AMD Opteron(tm) Processor 2214
; Should be something like:
; cpu_model = Dual-Core AMD Opteron(tm) Processor 2216
; Vendor's name -- AMD or Intel?
cpu_vendor = AMD
; Approximate speed, in MHZ, of the chips
cpu_speed_mhz = 2211
; Must be an integer.  Example: cpu_speed_mhz = 2400

; Number of CPUs (physical chips) per node
cpus_per_node = 2 
; Number of cores per node.
cores_per_node = 4 
; For a dual-socket quad-core, you would put cpus_per_node=2 and
; cores_per_node=8

; Set to true or false depending on inbound connectivity.  That is, external
; hosts can contact the worker nodes in this subcluster based on their hostname.
inbound_network = TRUE
; Set to true or false depending on outbound connectivity.  Set to true if the
; worker nodes in this subcluster can communicate with the external internet.
outbound_network = TRUE

; Here's a full example.  Remember, globally unique names!
; [Subcluster Dell Nodes UNL]
; name = Dell Nodes UNL
; node_count = 53
; ram_mb = 4110
; cpu_model = Dual-Core AMD Opteron(tm) Processor 2216
; cpu_vendor = AMD
; cpu_speed_mhz = 2400
; cpus_per_node = 2
; cores_per_node = 4
; inbound_network = FALSE
; outbound_network = TRUE


;===================================================================
;                             SE
;===================================================================

; For each storage element, add a new SE section.
; Each SE name must be unique for the entire grid, so make sure to not
; pick anything generic like "MAIN".  Each SE section must start with
; the words "SE", and cannot be named "CHANGEME".

; There are two main configuration types; one for dCache, one for BestMan

; For BestMan-based SEs.
;[SE CHANGEME]
; Name of the SE; set to be the same as the OIM registered name
;name = SE_CHANGEME
; The endpoint of the SE.  It MUST have the hostname, port, and the server
; location (/srm/v2/server in this case).  It MUST NOT have the ?SFN= string.
;srm_endpoint = httpg://srm.example.com:8443/srm/v2/server
; How to collect data; set to 'bestman' to allow it to query the endpoint for
; information or 'static' otherwise.
;provider_implementation = bestman
; Implementation name; bestman
;implementation = bestman
; Version number; the "bestman" provider implementation will attempt to auto-
; detect this also.
;version = 2.2.1.unknown
; Default paths for all of your VOs; VONAME is replaced with the VO's name.
;default_path = /mnt/bestman/home/VONAME
; Set to TRUE if the bestman provide can use 'df' on the directory referenced
; above to get the freespace information.  If set to false, it probably won't
; detect the correct info.
;use_df = True

; For dCache-based SEs
[SE dCache]
; Name of the SE; set to be the same as the OIM registered name
name = Purdue-CMS-SE
; The endpoint of the SE.  It MUST have the hostname, port, and the server
; location (/srm/managerv2 in this case).  It MUST NOT have the ?SFN= string.
srm_endpoint = httpg://dcache.rcac.purdue.edu:8443/srm/managerv2
; How to collect data; set to 'dcache' for dcache 1.8 (additional config req'd
; for this case), 'dcache19' for dcache 1.9, or 'static' for default values.
provider_implementation = static
; If you use the dcache provider, see http://twiki.grid.iu.edu/bin/view/InformationServices/DcacheGip
; If you use the dcache19 provider, you must fill in the location of your
; dCache's information provider:
;infoprovider_endpoint = http://dcache.example.com:2288/info.xml
; SE implementation name; leave as 'dcache'
implementation = dcache
advertise_se=False
; dCache version; non-static providers will also attempt to auto-detect this.
version = 1.8.0-15p6
; Default paths for all of your VOs; VONAME is replaced with the VO's name.
default_path = /pnfs/rcac.purdue.edu/data/VO/VONAME

; Here are working configs for BestMan and dCache
; [SE ddCache]
; name = T2_Nebraska_Storage
; srm_endpoint = httpg://srm.unl.edu:8443/srm/managerv2
; provider_implementation = static
; implementation = dcache
; version = 1.8.0-15p6
; default_path = /pnfs/unl.edu/data4/VONAME

; [SE Hadoop]
; name = T2_Nebraska_Hadoop
; srm_endpoint = httpg://dcache07.unl.edu:8443/srm/v2/server
; provider_implementation = bestman
; implementation = bestman
; version = 2.2.1.2.e1
; default_path = /user/VONAME


;===================================================================
;                            Install Locations
;===================================================================

[Install Locations]
; The osg option is used to give the location of the directory where the 
; osg ce software is installed
osg = /opt/osg

; The globus option is used to give the location of the directory where the 
; globus software is installed, it is the globus subdirectory of the osg 
; install location normally
globus = /opt/osg/globus

; This is the location of the file that contains the user vo map, it is usually
; the monitoring/osg-usr-vo-map.txt file in the osg install directory 
user_vo_map = %(osg_location)s/monitoring/osg-user-vo-map.txt

; This is the location of the file that contains the gridftp logs, it is usually
; the globus/var/log/gridftp.log file in the osg install directory 
gridftp_log = /opt/osg/globus/var/log/gridftp.log
